{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install motmetrics\n",
        "!pip install deep-sort-realtime\n",
        "!git clone https://github.com/abewley/sort.git\n",
        "!pip install filterpy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZZBIC9IBmOr",
        "outputId": "9f81c7e9-71d9-44c3-8c98-8da6d7690c03",
        "collapsed": true
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: motmetrics in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.11/dist-packages (from motmetrics) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from motmetrics) (2.2.2)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from motmetrics) (1.14.1)\n",
            "Requirement already satisfied: xmltodict>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from motmetrics) (0.14.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.1->motmetrics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.1->motmetrics) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.1->motmetrics) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=0.23.1->motmetrics) (1.17.0)\n",
            "Requirement already satisfied: deep-sort-realtime in /usr/local/lib/python3.11/dist-packages (1.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from deep-sort-realtime) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from deep-sort-realtime) (1.14.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from deep-sort-realtime) (4.11.0.86)\n",
            "fatal: destination path 'sort' already exists and is not an empty directory.\n",
            "Requirement already satisfied: filterpy in /usr/local/lib/python3.11/dist-packages (1.4.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from filterpy) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from filterpy) (1.14.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from filterpy) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->filterpy) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Imports</h3>"
      ],
      "metadata": {
        "id": "oS2WQBbNOqPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "matplotlib.use('Agg')  # non-interactive backend suitable for headless environments\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import gdown\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import motmetrics as mm\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import sys\n",
        "\n",
        "# Fix sort.py backend issue\n",
        "with open('/content/sort/sort.py', 'r') as file:\n",
        "    code = file.read()\n",
        "\n",
        "# Replace TkAgg with Agg\n",
        "fixed_code = code.replace('TkAgg', 'Agg')\n",
        "\n",
        "with open('/content/sort/sort.py', 'w') as file:\n",
        "    file.write(fixed_code)\n",
        "\n",
        "print(\"Backend issue fixed!\")\n",
        "\n",
        "sys.path.append('/content/sort')\n"
      ],
      "metadata": {
        "id": "DdDK0YteBk8d",
        "outputId": "b7aebe7f-5138-4c10-f4f2-1c3f12fe2cda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backend issue fixed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Configuration & Device Setup</h3>"
      ],
      "metadata": {
        "id": "hyHibUZxObhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURATION\n",
        "CONFIG = {\n",
        "    \"dataset_url\": \"https://drive.google.com/uc?id=1yvOwbPks7dFzMX2z4JoUQlwdEfNYQd7-\",\n",
        "    \"dataset_zip\": \"/content/MOT15.zip\",\n",
        "    \"dataset_path\": \"/content/MOT15\",\n",
        "    \"tracking\": {\"iou_threshold\": 0.3, \"max_age\": 30},\n",
        "    \"training\": {\"epochs\": 1, \"batch_size\": 8, \"learning_rate\": 0.0001},\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "id": "wHcmNWlaOaul",
        "outputId": "6fd911da-fee7-4c83-ef76-f1ee0bdb598a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Dataset Download & Extraction Functions</h3>"
      ],
      "metadata": {
        "id": "z5tnOYP6OeFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DOWNLOAD & EXTRACT DATASET\n",
        "def download_dataset():\n",
        "    if not os.path.exists(CONFIG[\"dataset_zip\"]):\n",
        "        print(\"Downloading MOT15 dataset from Google Drive...\")\n",
        "        gdown.download(CONFIG[\"dataset_url\"], CONFIG[\"dataset_zip\"], quiet=False)\n",
        "    else:\n",
        "        print(\"Dataset already downloaded.\")\n",
        "\n",
        "def extract_dataset():\n",
        "    if not os.path.exists(CONFIG[\"dataset_path\"]):\n",
        "        print(\"Extracting dataset...\")\n",
        "        with zipfile.ZipFile(CONFIG[\"dataset_zip\"], 'r') as zip_ref:\n",
        "            zip_ref.extractall(\"/content/\")\n",
        "        print(f\"Dataset extracted to {CONFIG['dataset_path']}\")\n",
        "    else:\n",
        "        print(\"Dataset already extracted.\")\n"
      ],
      "metadata": {
        "id": "M7_zZPQ_Od9b"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Data Augmentation Function</h3>"
      ],
      "metadata": {
        "id": "dQnWc1v1PXZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA AUGMENTATION\n",
        "def apply_augmentations(image):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((640, 640)),\n",
        "        transforms.RandomCrop(600),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 2.0)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    return transform(image)\n"
      ],
      "metadata": {
        "id": "junlfzkIOd6V"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>MOT15Dataset Class</h3>"
      ],
      "metadata": {
        "id": "KMgKPdwxPU2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MOT15 DATASET CLASS\n",
        "class MOT15Dataset(Dataset):\n",
        "    def __init__(self, root_dir, mode=\"train\", transform=None):\n",
        "        self.root_dir = os.path.join(root_dir, mode)\n",
        "        self.transform = transform\n",
        "        self.data = []\n",
        "        for seq in os.listdir(self.root_dir):\n",
        "            img_dir = os.path.join(self.root_dir, seq, \"img1\")\n",
        "            gt_path = os.path.join(self.root_dir, seq, \"gt/gt.txt\")\n",
        "            if os.path.exists(gt_path):\n",
        "                gt_df = pd.read_csv(gt_path, header=None)\n",
        "                gt_df.columns = [\"frame\", \"track_id\", \"x\", \"y\", \"w\", \"h\", \"conf\", \"class\", \"visibility\", \"unused\"]\n",
        "                for img_name in sorted(os.listdir(img_dir)):\n",
        "                    frame_id = int(img_name.split('.')[0])\n",
        "                    frame_gt = gt_df[gt_df[\"frame\"] == frame_id]\n",
        "                    boxes_df = frame_gt[[\"x\", \"y\", \"w\", \"h\"]].copy()\n",
        "                    boxes_df = pd.DataFrame({\n",
        "                        'x1': boxes_df['x'],\n",
        "                        'y1': boxes_df['y'],\n",
        "                        'x2': boxes_df['x'] + boxes_df['w'],\n",
        "                        'y2': boxes_df['y'] + boxes_df['h']\n",
        "                    })\n",
        "                    boxes = boxes_df[['x1', 'y1', 'x2', 'y2']].values\n",
        "                    labels = np.ones(len(boxes))\n",
        "                    self.data.append((os.path.join(img_dir, img_name), boxes, labels))\n",
        "            else:\n",
        "              # If no ground truth is available, load the image with empty boxes and labels.\n",
        "              for img_name in sorted(os.listdir(img_dir)):\n",
        "                  self.data.append((os.path.join(img_dir, img_name), np.empty((0, 4)), np.empty((0,))))\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, boxes, labels = self.data[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        target = {\"boxes\": torch.tensor(boxes, dtype=torch.float32), \"labels\": torch.tensor(labels, dtype=torch.int64)}\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, target\n"
      ],
      "metadata": {
        "id": "qtlucLyOOd3T"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Object Detector Class</h3>"
      ],
      "metadata": {
        "id": "FHpozzwxPg7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OBJECT DETECTOR CLASS\n",
        "class ObjectDetector:\n",
        "    def __init__(self, num_classes=2):\n",
        "        self.model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "        in_features = self.model.roi_heads.box_predictor.cls_score.in_features\n",
        "        self.model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "        self.model.to(device)\n",
        "        self.model.train()\n",
        "\n",
        "    def detect_objects(self, images):\n",
        "        img_tensors = [\n",
        "            img.to(device) if isinstance(img, torch.Tensor) else apply_augmentations(img).to(device)\n",
        "            for img in images\n",
        "        ]\n",
        "        with torch.no_grad():\n",
        "            predictions = self.model(img_tensors)\n",
        "        return predictions\n",
        "\n"
      ],
      "metadata": {
        "id": "F_fcnxF6Od0R"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Adaptive Tracker Class</h3>"
      ],
      "metadata": {
        "id": "GOTKtPDdPp-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ADAPTIVE TRACKER CLASS\n",
        "# Make sure that Sort is imported from your fixed sort.py file.\n",
        "from sort import Sort\n",
        "\n",
        "class AdaptiveTracker:\n",
        "    def __init__(self):\n",
        "        self.sort_tracker = Sort()\n",
        "        # self.deep_sort = DeepSort(max_age=30, n_init=3, max_cosine_distance=0.2)\n",
        "        self.previous_tracks = {}\n",
        "\n",
        "    def track_objects(self, raw_detections, frame):\n",
        "        if len(raw_detections) > 0:\n",
        "            sort_dets = np.array([d[0] + [d[1]] for d in raw_detections])\n",
        "            sort_tracked = self.sort_tracker.update(sort_dets)\n",
        "        else:\n",
        "            sort_tracked = np.empty((0, 5))\n",
        "\n",
        "        # For a simpler pipeline, use only SORT results:\n",
        "        consistent_tracks = []\n",
        "        for track in sort_tracked:\n",
        "            track_id = int(track[4])\n",
        "            bbox = track[:4].tolist()\n",
        "            # Here you can add consistency checks if needed\n",
        "            consistent_tracks.append({'track_id': track_id, 'bbox': bbox})\n",
        "\n",
        "        return sort_tracked, consistent_tracks\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BqCtDofvOdxR"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Training & Evaluation Functions</h3>"
      ],
      "metadata": {
        "id": "JwkTT-oNPt_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING FUNCTION\n",
        "def train_faster_rcnn(model, train_loader, epochs=10, lr=0.0001):\n",
        "    optimizer = torch.optim.Adam(model.model.parameters(), lr=lr)\n",
        "    model.model.train()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for images, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            images = [img.to(device) for img in images]\n",
        "            targets = [{\"boxes\": t[\"boxes\"].to(device), \"labels\": t[\"labels\"].to(device)} for t in targets]\n",
        "            optimizer.zero_grad()\n",
        "            loss_dict = model.model(images, targets)\n",
        "            loss = sum(loss for loss in loss_dict.values())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "# PERFORMANCE EVALUATION FUNCTION\n",
        "def evaluate_performance(detections, dataset):\n",
        "    acc = mm.MOTAccumulator(auto_id=True)\n",
        "    for idx, det in enumerate(detections):\n",
        "        print(f\"Frame {idx}: Detected track IDs: {det['track_id']}, Bounding boxes: {det['bboxes']}\")\n",
        "        print(\"Detected track IDs:\", det[\"track_id\"])\n",
        "        print(\"Detected bboxes:\", det[\"bboxes\"])\n",
        "        gt_boxes = dataset[idx][1][\"boxes\"].numpy()\n",
        "        gt_ids = np.arange(len(gt_boxes))\n",
        "        det_boxes = np.array(det[\"bboxes\"])\n",
        "        det_ids = det[\"track_id\"]\n",
        "        distances = mm.distances.iou_matrix(gt_boxes, det_boxes, max_iou=0.3)\n",
        "        acc.update(gt_ids, det_ids, distances)\n",
        "\n",
        "    mh = mm.metrics.create()\n",
        "    summary = mh.compute(acc, metrics=['mota', 'motp', 'idf1', 'num_switches'], name='Overall')\n",
        "    print(summary)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n"
      ],
      "metadata": {
        "id": "JRvHBCPjOduV"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Training Execution</h3>"
      ],
      "metadata": {
        "id": "t2xXWubbPyKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# Training Cell\n",
        "# ------------------------------\n",
        "\n",
        "# Download and extract dataset\n",
        "download_dataset()\n",
        "extract_dataset()\n",
        "\n",
        "# Create datasets and dataloaders (using training augmentation)\n",
        "train_dataset = MOT15Dataset(CONFIG[\"dataset_path\"], mode=\"train\", transform=apply_augmentations)\n",
        "test_dataset = MOT15Dataset(CONFIG[\"dataset_path\"], mode=\"test\", transform=apply_augmentations)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"training\"][\"batch_size\"],\n",
        "                          shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=CONFIG[\"training\"][\"batch_size\"],\n",
        "                         shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Initialize and train detector\n",
        "detector = ObjectDetector(num_classes=2)\n",
        "train_faster_rcnn(detector, train_loader, epochs=CONFIG[\"training\"][\"epochs\"],\n",
        "                  lr=CONFIG[\"training\"][\"learning_rate\"])\n",
        "\n",
        "# Save the trained model checkpoint\n",
        "torch.save(detector.model.state_dict(), \"/content/fasterrcnn_checkpoint.pth\")\n",
        "print(\"Model checkpoint saved!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "MprJ7No5f_yw",
        "outputId": "3a4c7fa1-c8fc-496a-a7e6-56cd874026d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset already downloaded.\n",
            "Dataset already extracted.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1: 100%|██████████| 688/688 [25:05<00:00,  2.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 4235.9048\n",
            "Model checkpoint saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "# Split the training dataset (assuming 'train_dataset' is already defined)\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "print(f\"Train subset size: {len(train_subset)}\")\n",
        "print(f\"Validation subset size: {len(val_subset)}\")\n",
        "\n",
        "# Create a DataLoader for the validation subset\n",
        "val_loader = DataLoader(val_subset, batch_size=CONFIG[\"training\"][\"batch_size\"],\n",
        "                        shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Switch model to evaluation mode (if not already)\n",
        "detector.model.eval()\n",
        "\n",
        "all_val_detections = []\n",
        "frame_counter = 0\n",
        "\n",
        "for images, _ in tqdm(val_loader, desc=\"Running Validation\"):\n",
        "    # Get predictions for the current batch (list of dicts, one per image)\n",
        "    predictions = detector.detect_objects(images)\n",
        "\n",
        "    for pred in predictions:\n",
        "        if len(pred[\"boxes\"]) > 0:\n",
        "            boxes = pred[\"boxes\"].cpu().numpy()\n",
        "            scores = pred[\"scores\"].cpu().numpy().reshape(-1, 1)\n",
        "            dets_array = np.hstack((boxes, scores))\n",
        "            # Format each detection as: ([x1,y1,x2,y2], score, dummy_class)\n",
        "            detections_list = [(d[:4].tolist(), float(d[4]), 0) for d in dets_array]\n",
        "        else:\n",
        "            detections_list = []\n",
        "\n",
        "        # If you're using a tracker, pass the corresponding image frame if needed.\n",
        "        # Here, we'll assume your tracker can work with the current image frame.\n",
        "        # For simplicity, if you're not using the tracker, you can skip this.\n",
        "        _, consistent_tracks = tracker.track_objects(detections_list, images[0])\n",
        "\n",
        "        # Save the tracker output for this frame\n",
        "        all_val_detections.append({\n",
        "            \"track_id\": [t.track_id for t in consistent_tracks] if isinstance(consistent_tracks[0], object) else [t[\"track_id\"] for t in consistent_tracks],\n",
        "            \"bboxes\": [t.to_tlbr() for t in consistent_tracks] if hasattr(consistent_tracks[0], \"to_tlbr\") else [t[\"bbox\"] for t in consistent_tracks]\n",
        "        })\n",
        "        frame_counter += 1\n"
      ],
      "metadata": {
        "id": "7lmVIHnxO-IH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Evaluation Cell</h3>"
      ],
      "metadata": {
        "id": "Y0FL0-TnKB-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# Evaluation Cell\n",
        "# ------------------------------\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the trained model checkpoint for evaluation\n",
        "detector = ObjectDetector(num_classes=2)\n",
        "detector.model.load_state_dict(torch.load(\"/content/fasterrcnn_checkpoint.pth\"))\n",
        "detector.model.eval()  # Set model to evaluation mode\n",
        "print(\"Model loaded for evaluation!\")\n",
        "\n",
        "# Re-create the test dataset (using the same transform as training)\n",
        "test_dataset = MOT15Dataset(CONFIG[\"dataset_path\"], mode=\"test\", transform=apply_augmentations)\n",
        "print(\"Test dataset size:\", len(test_dataset))\n",
        "\n",
        "# Debug: print a few samples from the test dataset\n",
        "for idx in range(5):\n",
        "    image, target = test_dataset[idx]\n",
        "    print(f\"Frame {idx}:\")\n",
        "\n",
        "    # Check the image type and size\n",
        "    if isinstance(image, torch.Tensor):\n",
        "        # Convert tensor to numpy (assuming image is in CxHxW format)\n",
        "        img_np = image.detach().cpu().permute(1, 2, 0).numpy()\n",
        "        print(\" - Image shape (HxWxC):\", img_np.shape)\n",
        "    else:\n",
        "        print(\" - Image size:\", image.size)\n",
        "\n",
        "    # Print ground truth boxes\n",
        "    if \"boxes\" in target:\n",
        "        print(\" - Ground truth boxes:\", target[\"boxes\"])\n",
        "    else:\n",
        "        print(\" - No ground truth boxes found.\")\n",
        "\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "\n",
        "# # Run detection on the test dataset\n",
        "# tracker = AdaptiveTracker()\n",
        "# all_detections = []\n",
        "# frame_counter = 0\n",
        "\n",
        "# for images, _ in tqdm(test_loader, desc=\"Running Detection\"):\n",
        "#     # Get predictions for the current batch (list of dicts, one per image)\n",
        "#     predictions = detector.detect_objects(images)\n",
        "\n",
        "#     current_frame = np.array(images[0])\n",
        "\n",
        "#     # Process each image individually\n",
        "#     for pred in predictions:\n",
        "#         if len(pred[\"boxes\"]) > 0:\n",
        "#             # Convert boxes and scores into a numpy array (N,5)\n",
        "#             boxes = pred[\"boxes\"].cpu().numpy()         # shape (N,4)\n",
        "#             scores = pred[\"scores\"].cpu().numpy().reshape(-1, 1)  # shape (N,1)\n",
        "#             dets_array = np.hstack((boxes, scores))       # shape (N,5)\n",
        "#             # Convert each detection to expected format: ([x1,y1,x2,y2], score, dummy_class)\n",
        "#             detections_list = [(d[:4].tolist(), float(d[4]), 0) for d in dets_array]\n",
        "#         else:\n",
        "#             detections_list = []\n",
        "\n",
        "#         # Pass detections for the current image to the tracker (using frame_counter as frame id)\n",
        "#         _, consistent_tracks = tracker.track_objects(detections_list, current_frame)\n",
        "\n",
        "#         # Save the tracker output for this frame\n",
        "#         all_detections.append({\n",
        "#             \"track_id\": [t[\"track_id\"] for t in consistent_tracks],\n",
        "#             \"bboxes\": [t[\"bbox\"] for t in consistent_tracks]\n",
        "#         })\n",
        "\n",
        "#         frame_counter += 1\n",
        "\n",
        "# # Evaluate performance using the detections from this run\n",
        "# evaluate_performance(all_detections, test_dataset)\n",
        "# print(\"Evaluation completed!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "zEtENopygF0r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a36b885-6ff6-443f-93c3-fa36c3cb4b85"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-61-582e4ac4992b>:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  detector.model.load_state_dict(torch.load(\"/content/fasterrcnn_checkpoint.pth\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded for evaluation!\n",
            "Test dataset size: 5783\n",
            "Frame 0:\n",
            " - Image shape (HxWxC): (600, 600, 3)\n",
            " - Ground truth boxes: tensor([], size=(0, 4))\n",
            "----------------------------------------\n",
            "Frame 1:\n",
            " - Image shape (HxWxC): (600, 600, 3)\n",
            " - Ground truth boxes: tensor([], size=(0, 4))\n",
            "----------------------------------------\n",
            "Frame 2:\n",
            " - Image shape (HxWxC): (600, 600, 3)\n",
            " - Ground truth boxes: tensor([], size=(0, 4))\n",
            "----------------------------------------\n",
            "Frame 3:\n",
            " - Image shape (HxWxC): (600, 600, 3)\n",
            " - Ground truth boxes: tensor([], size=(0, 4))\n",
            "----------------------------------------\n",
            "Frame 4:\n",
            " - Image shape (HxWxC): (600, 600, 3)\n",
            " - Ground truth boxes: tensor([], size=(0, 4))\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "b6NlE39SOcvi"
      }
    }
  ]
}