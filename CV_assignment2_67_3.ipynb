{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install motmetrics\n",
        "!pip install deep-sort-realtime\n",
        "!git clone https://github.com/abewley/sort.git\n",
        "!pip install filterpy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZZBIC9IBmOr",
        "outputId": "74516513-d2b0-4e85-b3c5-2efd4c99f2bd",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting motmetrics\n",
            "  Downloading motmetrics-1.4.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.11/dist-packages (from motmetrics) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from motmetrics) (2.2.2)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from motmetrics) (1.14.1)\n",
            "Collecting xmltodict>=0.12.0 (from motmetrics)\n",
            "  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.1->motmetrics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.1->motmetrics) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.1->motmetrics) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=0.23.1->motmetrics) (1.17.0)\n",
            "Downloading motmetrics-1.4.0-py3-none-any.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.5/161.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: xmltodict, motmetrics\n",
            "Successfully installed motmetrics-1.4.0 xmltodict-0.14.2\n",
            "Collecting deep-sort-realtime\n",
            "  Downloading deep_sort_realtime-1.3.2-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from deep-sort-realtime) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from deep-sort-realtime) (1.14.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from deep-sort-realtime) (4.11.0.86)\n",
            "Downloading deep_sort_realtime-1.3.2-py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deep-sort-realtime\n",
            "Successfully installed deep-sort-realtime-1.3.2\n",
            "Cloning into 'sort'...\n",
            "remote: Enumerating objects: 208, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 208 (delta 2), reused 1 (delta 1), pack-reused 203 (from 2)\u001b[K\n",
            "Receiving objects: 100% (208/208), 1.20 MiB | 3.19 MiB/s, done.\n",
            "Resolving deltas: 100% (74/74), done.\n",
            "Collecting filterpy\n",
            "  Downloading filterpy-1.4.5.zip (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from filterpy) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from filterpy) (1.14.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from filterpy) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->filterpy) (1.17.0)\n",
            "Building wheels for collected packages: filterpy\n",
            "  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110458 sha256=212233aa413f3b33c465839a8c498656e7e9c4ebdcdd9e730fd9744f8e63f2ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/dc/3c/e12983eac132d00f82a20c6cbe7b42ce6e96190ef8fa2d15e1\n",
            "Successfully built filterpy\n",
            "Installing collected packages: filterpy\n",
            "Successfully installed filterpy-1.4.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Imports</h3>"
      ],
      "metadata": {
        "id": "oS2WQBbNOqPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "matplotlib.use('Agg')  # non-interactive backend suitable for headless environments\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import gdown\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import motmetrics as mm\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import sys\n",
        "\n",
        "# Fix sort.py backend issue\n",
        "with open('/content/sort/sort.py', 'r') as file:\n",
        "    code = file.read()\n",
        "\n",
        "# Replace TkAgg with Agg\n",
        "fixed_code = code.replace('TkAgg', 'Agg')\n",
        "\n",
        "with open('/content/sort/sort.py', 'w') as file:\n",
        "    file.write(fixed_code)\n",
        "\n",
        "print(\"Backend issue fixed!\")\n",
        "\n",
        "sys.path.append('/content/sort')\n"
      ],
      "metadata": {
        "id": "DdDK0YteBk8d",
        "outputId": "a546bfd1-ff0b-40db-c787-4f9a1aebb761",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backend issue fixed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Configuration & Device Setup</h3>"
      ],
      "metadata": {
        "id": "hyHibUZxObhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURATION\n",
        "CONFIG = {\n",
        "    \"dataset_url\": \"https://drive.google.com/uc?id=1yvOwbPks7dFzMX2z4JoUQlwdEfNYQd7-\",\n",
        "    \"dataset_zip\": \"/content/MOT15.zip\",\n",
        "    \"dataset_path\": \"/content/MOT15\",\n",
        "    \"tracking\": {\"iou_threshold\": 0.3, \"max_age\": 30},\n",
        "    \"training\": {\"epochs\": 1, \"batch_size\": 8, \"learning_rate\": 0.0001},\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "id": "wHcmNWlaOaul",
        "outputId": "8259867b-93b0-49fe-8688-841354ccae59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Dataset Download & Extraction Functions</h3>"
      ],
      "metadata": {
        "id": "z5tnOYP6OeFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DOWNLOAD & EXTRACT DATASET\n",
        "def download_dataset():\n",
        "    if not os.path.exists(CONFIG[\"dataset_zip\"]):\n",
        "        print(\"Downloading MOT15 dataset from Google Drive...\")\n",
        "        gdown.download(CONFIG[\"dataset_url\"], CONFIG[\"dataset_zip\"], quiet=False)\n",
        "    else:\n",
        "        print(\"Dataset already downloaded.\")\n",
        "\n",
        "def extract_dataset():\n",
        "    if not os.path.exists(CONFIG[\"dataset_path\"]):\n",
        "        print(\"Extracting dataset...\")\n",
        "        with zipfile.ZipFile(CONFIG[\"dataset_zip\"], 'r') as zip_ref:\n",
        "            zip_ref.extractall(\"/content/\")\n",
        "        print(f\"Dataset extracted to {CONFIG['dataset_path']}\")\n",
        "    else:\n",
        "        print(\"Dataset already extracted.\")\n"
      ],
      "metadata": {
        "id": "M7_zZPQ_Od9b"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Data Augmentation Function</h3>"
      ],
      "metadata": {
        "id": "dQnWc1v1PXZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA AUGMENTATION\n",
        "def apply_augmentations(image):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((640, 640)),\n",
        "        transforms.RandomCrop(600),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 2.0)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    return transform(image)\n"
      ],
      "metadata": {
        "id": "junlfzkIOd6V"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>MOT15Dataset Class</h3>"
      ],
      "metadata": {
        "id": "KMgKPdwxPU2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MOT15 DATASET CLASS\n",
        "class MOT15Dataset(Dataset):\n",
        "    def __init__(self, root_dir, mode=\"train\", transform=None):\n",
        "        self.root_dir = os.path.join(root_dir, mode)\n",
        "        self.transform = transform\n",
        "        self.data = []\n",
        "        for seq in os.listdir(self.root_dir):\n",
        "            img_dir = os.path.join(self.root_dir, seq, \"img1\")\n",
        "            gt_path = os.path.join(self.root_dir, seq, \"gt/gt.txt\")\n",
        "            if os.path.exists(gt_path):\n",
        "                gt_df = pd.read_csv(gt_path, header=None)\n",
        "                gt_df.columns = [\"frame\", \"track_id\", \"x\", \"y\", \"w\", \"h\", \"conf\", \"class\", \"visibility\", \"unused\"]\n",
        "                for img_name in sorted(os.listdir(img_dir)):\n",
        "                    frame_id = int(img_name.split('.')[0])\n",
        "                    frame_gt = gt_df[gt_df[\"frame\"] == frame_id]\n",
        "                    boxes_df = frame_gt[[\"x\", \"y\", \"w\", \"h\"]].copy()\n",
        "                    boxes_df = pd.DataFrame({\n",
        "                        'x1': boxes_df['x'],\n",
        "                        'y1': boxes_df['y'],\n",
        "                        'x2': boxes_df['x'] + boxes_df['w'],\n",
        "                        'y2': boxes_df['y'] + boxes_df['h']\n",
        "                    })\n",
        "                    boxes = boxes_df[['x1', 'y1', 'x2', 'y2']].values\n",
        "                    labels = np.ones(len(boxes))\n",
        "                    self.data.append((os.path.join(img_dir, img_name), boxes, labels))\n",
        "            else:\n",
        "              # If no ground truth is available, load the image with empty boxes and labels.\n",
        "              for img_name in sorted(os.listdir(img_dir)):\n",
        "                  self.data.append((os.path.join(img_dir, img_name), np.empty((0, 4)), np.empty((0,))))\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, boxes, labels = self.data[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        target = {\"boxes\": torch.tensor(boxes, dtype=torch.float32), \"labels\": torch.tensor(labels, dtype=torch.int64)}\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, target\n"
      ],
      "metadata": {
        "id": "qtlucLyOOd3T"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Object Detector Class</h3>"
      ],
      "metadata": {
        "id": "FHpozzwxPg7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OBJECT DETECTOR CLASS\n",
        "class ObjectDetector:\n",
        "    def __init__(self, num_classes=2):\n",
        "        self.model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "        in_features = self.model.roi_heads.box_predictor.cls_score.in_features\n",
        "        self.model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "        self.model.to(device)\n",
        "        self.model.train()\n",
        "\n",
        "    def detect_objects(self, images):\n",
        "        img_tensors = [\n",
        "            img.to(device) if isinstance(img, torch.Tensor) else apply_augmentations(img).to(device)\n",
        "            for img in images\n",
        "        ]\n",
        "        with torch.no_grad():\n",
        "            predictions = self.model(img_tensors)\n",
        "        return predictions\n",
        "\n"
      ],
      "metadata": {
        "id": "F_fcnxF6Od0R"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Adaptive Tracker Class</h3>"
      ],
      "metadata": {
        "id": "GOTKtPDdPp-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ADAPTIVE TRACKER CLASS\n",
        "# Make sure that Sort is imported from your fixed sort.py file.\n",
        "from sort import Sort\n",
        "\n",
        "class AdaptiveTracker:\n",
        "    def __init__(self):\n",
        "        self.sort_tracker = Sort()\n",
        "        # self.deep_sort = DeepSort(max_age=30, n_init=3, max_cosine_distance=0.2)\n",
        "        self.previous_tracks = {}\n",
        "\n",
        "    def track_objects(self, raw_detections, frame):\n",
        "        if len(raw_detections) > 0:\n",
        "            sort_dets = np.array([d[0] + [d[1]] for d in raw_detections])\n",
        "            sort_tracked = self.sort_tracker.update(sort_dets)\n",
        "        else:\n",
        "            sort_tracked = np.empty((0, 5))\n",
        "\n",
        "        # For a simpler pipeline, use only SORT results:\n",
        "        consistent_tracks = []\n",
        "        for track in sort_tracked:\n",
        "            track_id = int(track[4])\n",
        "            bbox = track[:4].tolist()\n",
        "            # Here you can add consistency checks if needed\n",
        "            consistent_tracks.append({'track_id': track_id, 'bbox': bbox})\n",
        "\n",
        "        return sort_tracked, consistent_tracks\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BqCtDofvOdxR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Training & Evaluation Functions</h3>"
      ],
      "metadata": {
        "id": "JwkTT-oNPt_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING FUNCTION\n",
        "def train_faster_rcnn(model, train_loader, epochs=10, lr=0.0001):\n",
        "    optimizer = torch.optim.Adam(model.model.parameters(), lr=lr)\n",
        "    model.model.train()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for images, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            images = [img.to(device) for img in images]\n",
        "            targets = [{\"boxes\": t[\"boxes\"].to(device), \"labels\": t[\"labels\"].to(device)} for t in targets]\n",
        "            optimizer.zero_grad()\n",
        "            loss_dict = model.model(images, targets)\n",
        "            loss = sum(loss for loss in loss_dict.values())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "# PERFORMANCE EVALUATION FUNCTION\n",
        "def evaluate_performance(detections, dataset):\n",
        "    acc = mm.MOTAccumulator(auto_id=True)\n",
        "    for idx, det in enumerate(detections):\n",
        "        print(f\"Frame {idx}: Detected track IDs: {det['track_id']}, Bounding boxes: {det['bboxes']}\")\n",
        "        print(\"Detected track IDs:\", det[\"track_id\"])\n",
        "        print(\"Detected bboxes:\", det[\"bboxes\"])\n",
        "        gt_boxes = dataset[idx][1][\"boxes\"].numpy()\n",
        "        gt_ids = np.arange(len(gt_boxes))\n",
        "        det_boxes = np.array(det[\"bboxes\"])\n",
        "        det_ids = det[\"track_id\"]\n",
        "        distances = mm.distances.iou_matrix(gt_boxes, det_boxes, max_iou=0.3)\n",
        "        acc.update(gt_ids, det_ids, distances)\n",
        "\n",
        "    mh = mm.metrics.create()\n",
        "    summary = mh.compute(acc, metrics=['mota', 'motp', 'idf1', 'num_switches'], name='Overall')\n",
        "    print(summary)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n"
      ],
      "metadata": {
        "id": "JRvHBCPjOduV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Training Execution</h3>"
      ],
      "metadata": {
        "id": "t2xXWubbPyKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# Training Cell\n",
        "# ------------------------------\n",
        "\n",
        "# Download and extract dataset\n",
        "download_dataset()\n",
        "extract_dataset()\n",
        "\n",
        "# Create datasets and dataloaders (using training augmentation)\n",
        "train_dataset = MOT15Dataset(CONFIG[\"dataset_path\"], mode=\"train\", transform=apply_augmentations)\n",
        "test_dataset = MOT15Dataset(CONFIG[\"dataset_path\"], mode=\"test\", transform=apply_augmentations)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"training\"][\"batch_size\"],\n",
        "                          shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=CONFIG[\"training\"][\"batch_size\"],\n",
        "                         shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Initialize and train detector\n",
        "detector = ObjectDetector(num_classes=2)\n",
        "train_faster_rcnn(detector, train_loader, epochs=CONFIG[\"training\"][\"epochs\"],\n",
        "                  lr=CONFIG[\"training\"][\"learning_rate\"])\n",
        "\n",
        "# Save the trained model checkpoint\n",
        "torch.save(detector.model.state_dict(), \"/content/fasterrcnn_checkpoint.pth\")\n",
        "print(\"Model checkpoint saved!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "MprJ7No5f_yw",
        "outputId": "fe7d050d-78b4-41b8-bf2d-c7ada98c57be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading MOT15 dataset from Google Drive...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1yvOwbPks7dFzMX2z4JoUQlwdEfNYQd7-\n",
            "From (redirected): https://drive.google.com/uc?id=1yvOwbPks7dFzMX2z4JoUQlwdEfNYQd7-&confirm=t&uuid=20a72d8b-d7d6-42fb-b5cd-a72982a9c606\n",
            "To: /content/MOT15.zip\n",
            "100%|██████████| 1.31G/1.31G [00:23<00:00, 56.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset...\n",
            "Dataset extracted to /content/MOT15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
            "100%|██████████| 160M/160M [00:01<00:00, 158MB/s]\n",
            "Epoch 1/1: 100%|██████████| 688/688 [25:46<00:00,  2.25s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 4398.8666\n",
            "Model checkpoint saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "#########################################\n",
        "# Utility Functions\n",
        "#########################################\n",
        "\n",
        "from tabulate import tabulate\n",
        "def compute_iou(box1, box2):\n",
        "    \"\"\"\n",
        "    Compute Intersection over Union (IoU) between two boxes.\n",
        "    Boxes are in [x1, y1, x2, y2] format.\n",
        "    \"\"\"\n",
        "    x1 = max(box1[0], box2[0])\n",
        "    y1 = max(box1[1], box2[1])\n",
        "    x2 = min(box1[2], box2[2])\n",
        "    y2 = min(box1[3], box2[3])\n",
        "\n",
        "    inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
        "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "    union_area = area1 + area2 - inter_area\n",
        "\n",
        "    if union_area == 0:\n",
        "        return 0\n",
        "    return inter_area / union_area\n",
        "\n",
        "def evaluate_detector(data_loader, detector, iou_threshold=0.5, compute_metrics=True):\n",
        "    \"\"\"\n",
        "    Evaluate the detector over a given DataLoader.\n",
        "\n",
        "    If compute_metrics is True, this function computes precision, recall,\n",
        "    and F1-score based on the IoU between predicted boxes and ground-truth boxes.\n",
        "\n",
        "    It also measures the average inference time per frame.\n",
        "\n",
        "    Returns:\n",
        "        precision, recall, f1_score, avg_inference_time\n",
        "    \"\"\"\n",
        "    true_positives = 0\n",
        "    false_positives = 0\n",
        "    false_negatives = 0\n",
        "    total_frames = 0\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for images, targets in tqdm(data_loader, desc=\"Evaluating Detector\"):\n",
        "        # Get predictions for the current batch\n",
        "        predictions = detector.detect_objects(images)\n",
        "\n",
        "        # For each image in the batch:\n",
        "        for i, pred in enumerate(predictions):\n",
        "            if compute_metrics and targets is not None and \"boxes\" in targets[i] and targets[i][\"boxes\"].numel() > 0:\n",
        "                gt_boxes = targets[i][\"boxes\"].cpu().numpy()\n",
        "                if len(pred[\"boxes\"]) > 0:\n",
        "                    pred_boxes = pred[\"boxes\"].cpu().numpy()\n",
        "                else:\n",
        "                    pred_boxes = np.empty((0, 4))\n",
        "\n",
        "                matched_gt = set()\n",
        "                for pred_box in pred_boxes:\n",
        "                    match_found = False\n",
        "                    for j, gt_box in enumerate(gt_boxes):\n",
        "                        if j not in matched_gt and compute_iou(pred_box.tolist(), gt_box.tolist()) >= iou_threshold:\n",
        "                            true_positives += 1\n",
        "                            matched_gt.add(j)\n",
        "                            match_found = True\n",
        "                            break\n",
        "                    if not match_found:\n",
        "                        false_positives += 1\n",
        "\n",
        "                false_negatives += (len(gt_boxes) - len(matched_gt))\n",
        "            total_frames += 1\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    avg_inference_time = elapsed_time / total_frames if total_frames > 0 else 0\n",
        "\n",
        "    if compute_metrics:\n",
        "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
        "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
        "        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    else:\n",
        "        precision = recall = f1_score = 0\n",
        "\n",
        "    return precision, recall, f1_score, avg_inference_time\n",
        "\n",
        "#########################################\n",
        "# 1. Split Training Dataset into Validation Subset\n",
        "#########################################\n",
        "# Assume train_dataset, CONFIG, and collate_fn are defined in your notebook.\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "print(f\"Train subset size: {len(train_subset)}\")\n",
        "print(f\"Validation subset size: {len(val_subset)}\")\n",
        "\n",
        "# Create DataLoader for the validation subset\n",
        "val_loader = DataLoader(val_subset, batch_size=CONFIG[\"training\"][\"batch_size\"],\n",
        "                        shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "#########################################\n",
        "# 2. Set Detector to Evaluation Mode\n",
        "#########################################\n",
        "detector.model.eval()\n",
        "\n",
        "#########################################\n",
        "# 3. Evaluate on Validation Subset\n",
        "#########################################\n",
        "print(\"\\n--- Evaluating on Validation Subset ---\")\n",
        "precision, recall, f1_score, avg_time = evaluate_detector(val_loader, detector, iou_threshold=0.5, compute_metrics=True)\n",
        "metrics_table = [\n",
        "    [\"Precision\", f\"{precision:.4f}\"],\n",
        "    [\"Recall\", f\"{recall:.4f}\"],\n",
        "    [\"F1 Score\", f\"{f1_score:.4f}\"],\n",
        "    [\"Average Inference Time (s)\", f\"{avg_time:.4f}\"]\n",
        "]\n",
        "\n",
        "print(tabulate(metrics_table, headers=[\"Metric\", \"Value\"], tablefmt=\"github\"))\n",
        "\n",
        "\n",
        "#########################################\n",
        "# 4. (Optional) Evaluate on Test Dataset\n",
        "#########################################\n",
        "# Since the test dataset does not include ground-truth annotations,\n",
        "# we disable metric computation (compute_metrics=False) and only measure inference speed.\n",
        "print(\"\\n--- Evaluating on Test Dataset (Qualitative/Speed Only) ---\")\n",
        "test_dataset = MOT15Dataset(CONFIG[\"dataset_path\"], mode=\"test\", transform=apply_augmentations)\n",
        "print(\"Test dataset size:\", len(test_dataset))\n",
        "test_loader = DataLoader(test_dataset, batch_size=CONFIG[\"training\"][\"batch_size\"],\n",
        "                         shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# For test evaluation, we set compute_metrics to False because ground truth is not available.\n",
        "precision_test, recall_test, f1_test, avg_time_test = evaluate_detector(test_loader, detector, iou_threshold=0.5, compute_metrics=False)\n",
        "print(\"Test Dataset Evaluation:\")\n",
        "print(f\"Average Inference Time per Frame: {avg_time_test:.4f} seconds\")\n"
      ],
      "metadata": {
        "id": "VI1QttyEfbsN",
        "outputId": "f47ffdc0-598b-43fb-d5de-be08bbcf1322",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train subset size: 4400\n",
            "Validation subset size: 1100\n",
            "\n",
            "--- Evaluating on Validation Subset ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Detector: 100%|██████████| 138/138 [03:13<00:00,  1.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Metric                     |   Value |\n",
            "|----------------------------|---------|\n",
            "| Precision                  |  0.0327 |\n",
            "| Recall                     |  0.2745 |\n",
            "| F1 Score                   |  0.0585 |\n",
            "| Average Inference Time (s) |  0.1755 |\n",
            "\n",
            "--- Evaluating on Test Dataset (Qualitative/Speed Only) ---\n",
            "Test dataset size: 5783\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Detector:  93%|█████████▎| 670/723 [15:37<01:11,  1.36s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "b6NlE39SOcvi"
      }
    }
  ]
}