{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install motmetrics\n",
        "!pip install deep-sort-realtime\n",
        "!git clone https://github.com/abewley/sort.git\n",
        "!pip install filterpy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZZBIC9IBmOr",
        "outputId": "adbf529a-3e39-4799-b88a-b78cb63c8791"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting motmetrics\n",
            "  Downloading motmetrics-1.4.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.11/dist-packages (from motmetrics) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from motmetrics) (2.2.2)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from motmetrics) (1.13.1)\n",
            "Collecting xmltodict>=0.12.0 (from motmetrics)\n",
            "  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.1->motmetrics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.1->motmetrics) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.1->motmetrics) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=0.23.1->motmetrics) (1.17.0)\n",
            "Downloading motmetrics-1.4.0-py3-none-any.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.5/161.5 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: xmltodict, motmetrics\n",
            "Successfully installed motmetrics-1.4.0 xmltodict-0.14.2\n",
            "Collecting deep-sort-realtime\n",
            "  Downloading deep_sort_realtime-1.3.2-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from deep-sort-realtime) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from deep-sort-realtime) (1.13.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from deep-sort-realtime) (4.11.0.86)\n",
            "Downloading deep_sort_realtime-1.3.2-py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deep-sort-realtime\n",
            "Successfully installed deep-sort-realtime-1.3.2\n",
            "Cloning into 'sort'...\n",
            "remote: Enumerating objects: 208, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 208 (delta 2), reused 1 (delta 1), pack-reused 203 (from 2)\u001b[K\n",
            "Receiving objects: 100% (208/208), 1.20 MiB | 26.82 MiB/s, done.\n",
            "Resolving deltas: 100% (74/74), done.\n",
            "Collecting filterpy\n",
            "  Downloading filterpy-1.4.5.zip (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from filterpy) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from filterpy) (1.13.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from filterpy) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->filterpy) (1.17.0)\n",
            "Building wheels for collected packages: filterpy\n",
            "  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110458 sha256=c425308cd31744cc39d1a197e8e6cb2178c80d0c9172de570f3e4c41958d1323\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/dc/3c/e12983eac132d00f82a20c6cbe7b42ce6e96190ef8fa2d15e1\n",
            "Successfully built filterpy\n",
            "Installing collected packages: filterpy\n",
            "Successfully installed filterpy-1.4.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Imports</h3>"
      ],
      "metadata": {
        "id": "oS2WQBbNOqPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "matplotlib.use('Agg')  # non-interactive backend suitable for headless environments\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import gdown\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import motmetrics as mm\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import sys\n",
        "\n",
        "# Fix sort.py backend issue\n",
        "with open('/content/sort/sort.py', 'r') as file:\n",
        "    code = file.read()\n",
        "\n",
        "# Replace TkAgg with Agg\n",
        "fixed_code = code.replace('TkAgg', 'Agg')\n",
        "\n",
        "with open('/content/sort/sort.py', 'w') as file:\n",
        "    file.write(fixed_code)\n",
        "\n",
        "print(\"Backend issue fixed!\")\n",
        "\n",
        "sys.path.append('/content/sort')\n"
      ],
      "metadata": {
        "id": "DdDK0YteBk8d",
        "outputId": "594e443b-b1e2-4459-c53a-4a8b294145d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backend issue fixed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Configuration & Device Setup</h3>"
      ],
      "metadata": {
        "id": "hyHibUZxObhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURATION\n",
        "CONFIG = {\n",
        "    \"dataset_url\": \"https://drive.google.com/uc?id=1yvOwbPks7dFzMX2z4JoUQlwdEfNYQd7-\",\n",
        "    \"dataset_zip\": \"/content/MOT15.zip\",\n",
        "    \"dataset_path\": \"/content/MOT15\",\n",
        "    \"tracking\": {\"iou_threshold\": 0.3, \"max_age\": 30},\n",
        "    \"training\": {\"epochs\": 5, \"batch_size\": 8, \"learning_rate\": 0.0001},\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "id": "wHcmNWlaOaul",
        "outputId": "d36d607c-317a-4080-9f27-923b5e279a71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Dataset Download & Extraction Functions</h3>"
      ],
      "metadata": {
        "id": "z5tnOYP6OeFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DOWNLOAD & EXTRACT DATASET\n",
        "def download_dataset():\n",
        "    if not os.path.exists(CONFIG[\"dataset_zip\"]):\n",
        "        print(\"Downloading MOT15 dataset from Google Drive...\")\n",
        "        gdown.download(CONFIG[\"dataset_url\"], CONFIG[\"dataset_zip\"], quiet=False)\n",
        "    else:\n",
        "        print(\"Dataset already downloaded.\")\n",
        "\n",
        "def extract_dataset():\n",
        "    if not os.path.exists(CONFIG[\"dataset_path\"]):\n",
        "        print(\"Extracting dataset...\")\n",
        "        with zipfile.ZipFile(CONFIG[\"dataset_zip\"], 'r') as zip_ref:\n",
        "            zip_ref.extractall(\"/content/\")\n",
        "        print(f\"Dataset extracted to {CONFIG['dataset_path']}\")\n",
        "    else:\n",
        "        print(\"Dataset already extracted.\")\n"
      ],
      "metadata": {
        "id": "M7_zZPQ_Od9b"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Data Augmentation Function</h3>"
      ],
      "metadata": {
        "id": "dQnWc1v1PXZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA AUGMENTATION\n",
        "def apply_augmentations(image):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((640, 640)),\n",
        "        transforms.RandomCrop(600),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 2.0)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    return transform(image)\n"
      ],
      "metadata": {
        "id": "junlfzkIOd6V"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>MOT15Dataset Class</h3>"
      ],
      "metadata": {
        "id": "KMgKPdwxPU2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MOT15 DATASET CLASS\n",
        "class MOT15Dataset(Dataset):\n",
        "    def __init__(self, root_dir, mode=\"train\", transform=None):\n",
        "        self.root_dir = os.path.join(root_dir, mode)\n",
        "        self.transform = transform\n",
        "        self.data = []\n",
        "        for seq in os.listdir(self.root_dir):\n",
        "            img_dir = os.path.join(self.root_dir, seq, \"img1\")\n",
        "            gt_path = os.path.join(self.root_dir, seq, \"gt/gt.txt\")\n",
        "            if os.path.exists(gt_path):\n",
        "                gt_df = pd.read_csv(gt_path, header=None)\n",
        "                gt_df.columns = [\"frame\", \"track_id\", \"x\", \"y\", \"w\", \"h\", \"conf\", \"class\", \"visibility\", \"unused\"]\n",
        "                for img_name in sorted(os.listdir(img_dir)):\n",
        "                    frame_id = int(img_name.split('.')[0])\n",
        "                    frame_gt = gt_df[gt_df[\"frame\"] == frame_id]\n",
        "                    boxes_df = frame_gt[[\"x\", \"y\", \"w\", \"h\"]].copy()\n",
        "                    boxes_df = pd.DataFrame({\n",
        "                        'x1': boxes_df['x'],\n",
        "                        'y1': boxes_df['y'],\n",
        "                        'x2': boxes_df['x'] + boxes_df['w'],\n",
        "                        'y2': boxes_df['y'] + boxes_df['h']\n",
        "                    })\n",
        "                    boxes = boxes_df[['x1', 'y1', 'x2', 'y2']].values\n",
        "                    labels = np.ones(len(boxes))\n",
        "                    self.data.append((os.path.join(img_dir, img_name), boxes, labels))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, boxes, labels = self.data[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        target = {\"boxes\": torch.tensor(boxes, dtype=torch.float32), \"labels\": torch.tensor(labels, dtype=torch.int64)}\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, target\n"
      ],
      "metadata": {
        "id": "qtlucLyOOd3T"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Object Detector Class</h3>"
      ],
      "metadata": {
        "id": "FHpozzwxPg7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OBJECT DETECTOR CLASS\n",
        "class ObjectDetector:\n",
        "    def __init__(self, num_classes=2):\n",
        "        self.model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "        in_features = self.model.roi_heads.box_predictor.cls_score.in_features\n",
        "        self.model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "        self.model.to(device)\n",
        "        self.model.train()\n",
        "\n",
        "    def detect_objects(self, images):\n",
        "        img_tensors = [apply_augmentations(img).to(device) for img in images]\n",
        "        with torch.no_grad():\n",
        "            predictions = self.model(img_tensors)\n",
        "        return predictions\n"
      ],
      "metadata": {
        "id": "F_fcnxF6Od0R"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Adaptive Tracker Class</h3>"
      ],
      "metadata": {
        "id": "GOTKtPDdPp-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ADAPTIVE TRACKER CLASS\n",
        "# Make sure that Sort is imported from your fixed sort.py file.\n",
        "from sort import Sort\n",
        "\n",
        "class AdaptiveTracker:\n",
        "    def __init__(self):\n",
        "        self.deep_sort = DeepSort(max_age=30, n_init=3, max_cosine_distance=0.2)\n",
        "        self.sort_tracker = Sort()\n",
        "        self.previous_tracks = {}\n",
        "\n",
        "    def track_objects(self, detections, frame_num):\n",
        "        sort_tracked = self.sort_tracker.update(np.array(detections) if detections else np.empty((0, 5)))\n",
        "        deep_sort_tracked = self.deep_sort.update_tracks(detections, frame_num=frame_num)\n",
        "\n",
        "        consistent_tracks = []\n",
        "        for track in deep_sort_tracked:\n",
        "            track_id = track.track_id\n",
        "            bbox = track.to_tlbr()\n",
        "            if track_id in self.previous_tracks:\n",
        "                prev_bbox = self.previous_tracks[track_id]\n",
        "                if np.linalg.norm(np.array(bbox[:2]) - np.array(prev_bbox[:2])) < 50:\n",
        "                    consistent_tracks.append(track)\n",
        "            else:\n",
        "                consistent_tracks.append(track)\n",
        "            self.previous_tracks[track_id] = bbox\n",
        "\n",
        "        return sort_tracked, consistent_tracks\n"
      ],
      "metadata": {
        "id": "BqCtDofvOdxR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Training & Evaluation Functions</h3>"
      ],
      "metadata": {
        "id": "JwkTT-oNPt_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING FUNCTION\n",
        "def train_faster_rcnn(model, train_loader, epochs=10, lr=0.0001):\n",
        "    optimizer = torch.optim.Adam(model.model.parameters(), lr=lr)\n",
        "    model.model.train()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for images, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            images = [img.to(device) for img in images]\n",
        "            targets = [{\"boxes\": t[\"boxes\"].to(device), \"labels\": t[\"labels\"].to(device)} for t in targets]\n",
        "            optimizer.zero_grad()\n",
        "            loss_dict = model.model(images, targets)\n",
        "            loss = sum(loss for loss in loss_dict.values())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "# PERFORMANCE EVALUATION FUNCTION\n",
        "def evaluate_performance(detections, dataset):\n",
        "    acc = mm.MOTAccumulator(auto_id=True)\n",
        "    for idx, det in enumerate(detections):\n",
        "        print(f\"Frame {idx}: Detected track IDs: {det['track_id']}, Bounding boxes: {det['bboxes']}\")\n",
        "        print(\"Detected track IDs:\", det[\"track_id\"])\n",
        "        print(\"Detected bboxes:\", det[\"bboxes\"])\n",
        "        gt_boxes = dataset[idx][1][\"boxes\"].numpy()\n",
        "        gt_ids = np.arange(len(gt_boxes))\n",
        "        det_boxes = np.array(det[\"bboxes\"])\n",
        "        det_ids = det[\"track_id\"]\n",
        "        distances = mm.distances.iou_matrix(gt_boxes, det_boxes, max_iou=0.3)\n",
        "        acc.update(gt_ids, det_ids, distances)\n",
        "\n",
        "    mh = mm.metrics.create()\n",
        "    summary = mh.compute(acc, metrics=['mota', 'motp', 'idf1', 'num_switches'], name='Overall')\n",
        "    print(summary)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n"
      ],
      "metadata": {
        "id": "JRvHBCPjOduV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Main Execution</h3>"
      ],
      "metadata": {
        "id": "t2xXWubbPyKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "B8jVoGXPat5X",
        "outputId": "54c8c0f2-75e5-4087-ef30-a7d1b0a7e6f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Mar 10 05:26:13 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8             12W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# Training Cell\n",
        "# ------------------------------\n",
        "\n",
        "# Download and extract dataset\n",
        "download_dataset()\n",
        "extract_dataset()\n",
        "\n",
        "# Create datasets and dataloaders (using training augmentation)\n",
        "train_dataset = MOT15Dataset(CONFIG[\"dataset_path\"], mode=\"train\", transform=apply_augmentations)\n",
        "test_dataset = MOT15Dataset(CONFIG[\"dataset_path\"], mode=\"test\", transform=apply_augmentations)\n",
        "train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"training\"][\"batch_size\"],\n",
        "                          shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=CONFIG[\"training\"][\"batch_size\"],\n",
        "                         shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Initialize and train detector\n",
        "detector = ObjectDetector(num_classes=2)\n",
        "train_faster_rcnn(detector, train_loader, epochs=CONFIG[\"training\"][\"epochs\"],\n",
        "                  lr=CONFIG[\"training\"][\"learning_rate\"])\n",
        "\n",
        "# Save the trained model checkpoint\n",
        "torch.save(detector.model.state_dict(), \"/content/fasterrcnn_checkpoint.pth\")\n",
        "print(\"Model checkpoint saved!\")\n",
        "\n",
        "# (Optional) Run detection on the test set and save detections for later evaluation\n",
        "tracker = AdaptiveTracker()\n",
        "all_detections = []\n",
        "for frame_num, (images, _) in tqdm(enumerate(test_loader), desc=\"Running Detection\"):\n",
        "    detections = detector.detect_objects(images)\n",
        "    _, consistent_tracks = tracker.track_objects(detections, frame_num)\n",
        "    all_detections.append({\n",
        "        \"track_id\": [t.track_id for t in consistent_tracks],\n",
        "        \"bboxes\": [t.to_tlbr() for t in consistent_tracks]\n",
        "    })\n",
        "\n",
        "# Save the detections so that you don't have to run detection again\n",
        "import pickle\n",
        "with open(\"/content/all_detections.pkl\", \"wb\") as f:\n",
        "    pickle.dump(all_detections, f)\n",
        "print(\"Detections saved!\")\n"
      ],
      "metadata": {
        "id": "MprJ7No5f_yw",
        "outputId": "412286c3-4b1c-42b9-a6c0-85136dd91734",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading MOT15 dataset from Google Drive...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1yvOwbPks7dFzMX2z4JoUQlwdEfNYQd7-\n",
            "From (redirected): https://drive.google.com/uc?id=1yvOwbPks7dFzMX2z4JoUQlwdEfNYQd7-&confirm=t&uuid=ac148551-656c-4648-bec8-6a426ac01570\n",
            "To: /content/MOT15.zip\n",
            "100%|██████████| 1.31G/1.31G [00:13<00:00, 98.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset...\n",
            "Dataset extracted to /content/MOT15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
            "100%|██████████| 160M/160M [00:01<00:00, 85.5MB/s]\n",
            "Epoch 1/5: 100%|██████████| 688/688 [26:54<00:00,  2.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 4227.5952\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 688/688 [26:37<00:00,  2.32s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Loss: 2987.5696\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 688/688 [26:27<00:00,  2.31s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Loss: 2698.8139\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5:  40%|████      | 276/688 [10:40<16:17,  2.37s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# Evaluation Cell\n",
        "# ------------------------------\n",
        "\n",
        "# Load the trained model checkpoint for evaluation\n",
        "detector = ObjectDetector(num_classes=2)\n",
        "detector.model.load_state_dict(torch.load(\"/content/fasterrcnn_checkpoint.pth\"))\n",
        "detector.model.eval()  # Set to evaluation mode\n",
        "print(\"Model loaded for evaluation!\")\n",
        "\n",
        "# Load saved detections (or you can run detection again if needed)\n",
        "import pickle\n",
        "with open(\"/content/all_detections.pkl\", \"rb\") as f:\n",
        "    all_detections = pickle.load(f)\n",
        "print(\"Detections loaded!\")\n",
        "\n",
        "# (Optional) For evaluation, you might want to use a deterministic transform instead of training augmentations.\n",
        "# For example, you could define and use a simpler transform like:\n",
        "# def evaluation_transform(image):\n",
        "#     transform = transforms.Compose([\n",
        "#         transforms.Resize((640, 640)),\n",
        "#         transforms.ToTensor(),\n",
        "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "#     ])\n",
        "#     return transform(image)\n",
        "# And then recreate the test_dataset with this transform if needed.\n",
        "\n",
        "# Re-create the test dataset (using the same transform as during training here)\n",
        "test_dataset = MOT15Dataset(CONFIG[\"dataset_path\"], mode=\"test\", transform=apply_augmentations)\n",
        "\n",
        "# Evaluate the performance\n",
        "evaluate_performance(all_detections, test_dataset)\n",
        "print(\"Evaluation completed!\")\n"
      ],
      "metadata": {
        "id": "zEtENopygF0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Download and extract dataset\n",
        "    download_dataset()\n",
        "    extract_dataset()\n",
        "\n",
        "    # Create dataset and dataloaders\n",
        "    train_dataset = MOT15Dataset(CONFIG[\"dataset_path\"], mode=\"train\", transform=apply_augmentations)\n",
        "    test_dataset = MOT15Dataset(CONFIG[\"dataset_path\"], mode=\"test\", transform=apply_augmentations)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"training\"][\"batch_size\"], shuffle=True, collate_fn=collate_fn)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=CONFIG[\"training\"][\"batch_size\"], shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    # Initialize detector and tracker\n",
        "    detector = ObjectDetector(num_classes=2)\n",
        "    train_faster_rcnn(detector, train_loader, epochs=CONFIG[\"training\"][\"epochs\"], lr=CONFIG[\"training\"][\"learning_rate\"])\n",
        "\n",
        "    tracker = AdaptiveTracker()\n",
        "    all_detections = []\n",
        "    for frame_num, (images, _) in tqdm(enumerate(test_loader), desc=\"Evaluating\"):\n",
        "        detections = detector.detect_objects(images)\n",
        "        _, consistent_tracks = tracker.track_objects(detections, frame_num)\n",
        "        all_detections.append({\n",
        "            \"track_id\": [t.track_id for t in consistent_tracks],\n",
        "            \"bboxes\": [t.to_tlbr() for t in consistent_tracks]\n",
        "        })\n",
        "\n",
        "    evaluate_performance(all_detections, test_dataset)\n",
        "    print(\"Training & Tracking Completed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WixXs3cOdrX",
        "outputId": "96eb24ac-1bae-49c6-882c-c4f93e5ca75e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset already downloaded.\n",
            "Dataset already extracted.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 688/688 [23:52<00:00,  2.08s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 4105.1125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 688/688 [23:33<00:00,  2.06s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Loss: 2903.7199\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 688/688 [23:31<00:00,  2.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Loss: 2669.8037\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 688/688 [23:37<00:00,  2.06s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Loss: 2458.3566\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 688/688 [23:30<00:00,  2.05s/it]\n",
            "/usr/local/lib/python3.11/dist-packages/deep_sort_realtime/embedder/embedder_pytorch.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.model.load_state_dict(torch.load(model_wts_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Loss: 2339.7972\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         mota  motp  idf1  num_switches\n",
            "Overall   NaN   NaN   NaN             0\n",
            "Training & Tracking Completed!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "b6NlE39SOcvi"
      }
    }
  ]
}